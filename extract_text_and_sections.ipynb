{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04557ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from acl_anthology import Anthology\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa69f23-9b69-477d-968d-346d11b72f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "anthology = Anthology.from_repo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcca64d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following venues divide their main conf paper collections as \".long\" and \".short\" separately. need to gather them\n",
    "divided_venues = ['2021.acl', '2022.acl', '2023.acl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67e23ec2-0b7a-447a-8ed1-69b82eb6618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_paper_id(venue_index: str, Anthology=anthology):\n",
    "    for i in range(1, 10000):\n",
    "        try:\n",
    "            a = anthology.get(venue_index + '.'+ str(i))\n",
    "            b = anthology.get(venue_index + '.'+ str(i+1))\n",
    "            assert not (a is None and b is None)\n",
    "        except:\n",
    "            if venue_index == '2023.acl-long': \n",
    "                # ACL 2023 has a PC's report appened as the last long paper (doesn't follow the formatting)\n",
    "                # \"Program Chairs’ Report on Peer Review at ACL 2023\", https://aclanthology.org/2023.acl-long.911/\n",
    "                return i-2\n",
    "            return i-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d5d320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_full_text_and_save(venue_index: str, divided_venues=divided_venues, enable_notes=False, direct_write=True):\n",
    "    \n",
    "    if not os.path.exists(f'data/{venue_index}_extracted_full.json'):\n",
    "        \n",
    "        if venue_index in divided_venues:\n",
    "            out_col = extract_full_text_and_save(venue_index+'-long', direct_write=False) + extract_full_text_and_save(venue_index+'-short', direct_write=False)\n",
    "            if direct_write:\n",
    "                with open(f'data/{venue_index}_extracted_full.json', 'w', encoding='utf-8') as f2:\n",
    "                    json.dump(out_col, f2, indent=2)\n",
    "                return\n",
    "            else:\n",
    "                return out_col\n",
    "        \n",
    "        print(f'Extracting text for {venue_index} from scratch.')\n",
    "        max_paper_id = get_max_paper_id(venue_index)\n",
    "        print(f'max paper id: {max_paper_id}')\n",
    "        \n",
    "        out_col = []\n",
    "        for j in tqdm(range(1, max_paper_id + 1)):\n",
    "            if venue_index == '2020.emnlp-main' and j==401:\n",
    "                continue # The page is having a 404 error. this paper might have been retracted?\n",
    "            url = f'https://aclanthology.org/{venue_index}.{str(j)}.pdf'\n",
    "            try:\n",
    "                url_pdf_file = requests.get(url).content\n",
    "                pdf_io_bytes = BytesIO(url_pdf_file)\n",
    "                reader = PdfReader(pdf_io_bytes)\n",
    "            except:\n",
    "                print(f'Warning: Unable to fetch the PDF file of Paper {venue_index}.{str(j)}. The URL is not working and please validate. This paper will be skipped.')\n",
    "                continue \n",
    "            page_labels = reader.page_labels\n",
    "            text_col, section_names_col, text_by_sections = [], ['__START__'], []\n",
    "            found_ref_flag = False\n",
    "            \n",
    "            for i in range(len(reader.page_labels)):\n",
    "                page = reader.pages[i]\n",
    "                label = page_labels[i]\n",
    "                try:\n",
    "                    text = page.extract_text()\n",
    "                except:\n",
    "                    print(f\"Warning: Failed to extract text from {venue_index}.{str(j)} Page {i+1}. This page is skipped.\")\n",
    "                    continue\n",
    "                text.removesuffix(label) # remove the page number\n",
    "                text_col.append(text)\n",
    "                \n",
    "                section_pattern = re.compile(r'((\\n|[a-z)\\]\"]\\.|^)([1-9] [A-Z]|References).*[\\n$])') # Don't want to include sebsections (e.g., 3.1 XXX or A.1 XXX)\n",
    "                res = section_pattern.findall(text)\n",
    "                for item in res:\n",
    "                    # An item looks like ('\\n7 Conclusion\\n', '\\n', '7 C') or ('\\nReferences\\n', '\\n', 'References')\n",
    "                    name, header, key = item\n",
    "                    if 'et al.,' in name:\n",
    "                        continue\n",
    "                    if key == 'References':\n",
    "                        found_ref_flag = True\n",
    "                    name = name.removeprefix(header).strip()\n",
    "                    section_names_col.append(name)\n",
    "                    #print(name)\n",
    "                \n",
    "            output = '\\n'.join(text_col)\n",
    "            output = output.strip().split('\\n') # divide the full text into each line\n",
    "            \n",
    "            if len(output) > 1:\n",
    "                # remove the line-end hyphens and replace \\n with a space\n",
    "                output = ''.join([item[:-1] if len(item) > 0 and item[-1] == '-' else item+' ' for item in output])\n",
    "                # remove the formatted page footer (on the first page)\n",
    "                file_start_pattern = re.compile(r'Proceedings of the .* (Conference|Meeting) .*©[0-9]* Association for Computational Linguistics')\n",
    "                footer_search = file_start_pattern.search(output)\n",
    "                if footer_search is not None:\n",
    "                    #print(main_text_start_pos)\n",
    "                    footer_start_pos = footer_search.start()\n",
    "                    main_text_start_pos = footer_search.end()\n",
    "                    if main_text_start_pos < 5000:\n",
    "                        output = output[:footer_start_pos] + output[main_text_start_pos:]\n",
    "                    else:\n",
    "                        print(f'Warning: File footer matching might be problematic in {venue_index}.{str(j)}. The file will still be processed and included. Please check manually.')\n",
    "                else:\n",
    "                    if enable_notes:\n",
    "                        print(f'Note: Didn\\'t find footer in {venue_index}.{str(j)}. (Usually there is no need to worry since it is often because the first page indeed doesn\\'t contain the footer.) The file will still be processed and included. Please check manually.')\n",
    "                # divide by section\n",
    "                start_pos = 0\n",
    "                for i in range(1, len(section_names_col)):\n",
    "                    end_pos = output.find(section_names_col[i], start_pos)\n",
    "                    text_by_sections.append((section_names_col[i-1], output[start_pos:end_pos].strip()))\n",
    "                    #print(start_pos, end_pos, section_names_col[i-1])\n",
    "                    start_pos = end_pos + len(section_names_col[i])\n",
    "                if not found_ref_flag:\n",
    "                    # Attempt to search \"References \" in the last chunk as the approximate start of the reference\n",
    "                    last_text_chunk = output[start_pos:]\n",
    "                    approx_ref_pos = last_text_chunk.find('References ')\n",
    "                    if approx_ref_pos < 0:\n",
    "                        approx_ref_pos = last_text_chunk.find('Acknowledgements ')\n",
    "                    #print('HERE!')\n",
    "                    #print(last_text_chunk)\n",
    "                    if approx_ref_pos >= 0:\n",
    "                        found_ref_flag = True\n",
    "                        if enable_notes:\n",
    "                            print(f'Note: The Reference section of Paper {venue_index}.{str(j)} is approximated by searching in the last chunk. It might be different from the exact position.')\n",
    "                        text_by_sections.append((section_names_col[-1], last_text_chunk[:approx_ref_pos].strip()))\n",
    "                        text_by_sections.append((\"References\", last_text_chunk[approx_ref_pos:].strip()))\n",
    "                    else:\n",
    "                        text_by_sections.append((section_names_col[-1], last_text_chunk.strip()))\n",
    "                else:\n",
    "                    text_by_sections.append((section_names_col[-1], output[start_pos:].strip()))\n",
    "                #print(start_pos, end_pos, section_names_col[-1])\n",
    "            else:\n",
    "                output = ''\n",
    "                print(f'Warning: empty output in Paper {venue_index}.{str(j)}; this paper will be skipped.')\n",
    "                    \n",
    "            if not found_ref_flag:\n",
    "                print(f'Warning: didn\\'t find References section in Paper {venue_index}.{str(j)}')\n",
    "        \n",
    "            res_dict = {'id':j, 'text':None, 'by_chapter':{}}\n",
    "            if len(output):\n",
    "                res_dict['text'] = output\n",
    "                if not len(section_names_col):\n",
    "                    print(f'Warning: Failed to recognize sections in Paper {venue_index}.{str(j)}')\n",
    "                for item in text_by_sections:\n",
    "                    res_dict['by_chapter'][item[0]] = item[1]\n",
    "                out_col.append(res_dict)\n",
    "        \n",
    "        if direct_write:\n",
    "            with open(f'data/{venue_index}_extracted_full.json', 'w', encoding='utf-8') as f2:\n",
    "                json.dump(out_col, f2, indent=2)\n",
    "            return\n",
    "        else:\n",
    "            return out_col\n",
    "\n",
    "    else:\n",
    "        print(f'{venue_index} has already been processed and stored.')\n",
    "        if direct_write:\n",
    "            return\n",
    "        else:\n",
    "            with open(f'data/{venue_index}_extracted_full.json', 'r', encoding='utf-8') as f3:\n",
    "                out_col = json.load(f3)\n",
    "            return out_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb7c478e-170a-49cb-ad72-7694db73f35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020.acl-main\n",
      "2020.acl-main has already been processed and stored.\n",
      "Updating matched strings...\n",
      "\n",
      "2020.emnlp-main\n",
      "2020.emnlp-main has already been processed and stored.\n",
      "Updating matched strings...\n",
      "\n",
      "2021.naacl-main\n",
      "2021.naacl-main has already been processed and stored.\n",
      "Updating matched strings...\n",
      "\n",
      "2021.acl\n",
      "2021.acl has already been processed and stored.\n",
      "Updating matched strings...\n",
      "\n",
      "2021.emnlp-main\n",
      "2021.emnlp-main has already been processed and stored.\n",
      "Updating matched strings...\n",
      "\n",
      "2022.acl\n",
      "2022.acl has already been processed and stored.\n",
      "Updating matched strings...\n",
      "\n",
      "2022.naacl-main\n",
      "2022.naacl-main has already been processed and stored.\n",
      "Updating matched strings...\n",
      "\n",
      "2022.emnlp-main\n",
      "2022.emnlp-main has already been processed and stored.\n",
      "Updating matched strings...\n",
      "\n",
      "2023.acl\n",
      "2023.acl has already been processed and stored.\n",
      "Updating matched strings...\n",
      "\n",
      "2023.emnlp-main\n",
      "2023.emnlp-main has already been processed and stored.\n",
      "Updating matched strings...\n"
     ]
    }
   ],
   "source": [
    "venue_list = [ # Annotations here: Paper submission deadlines\n",
    "    '2020.acl-main', # Dec 9, 2019\n",
    "    '2020.emnlp-main', # Jun 3, 2020\n",
    "    '2021.naacl-main', # Nov 23, 2020\n",
    "    '2021.acl', # Feb 2, 2021\n",
    "    '2021.emnlp-main', # May 17, 2021\n",
    "    '2022.acl', # Nov 15, 2021\n",
    "    '2022.naacl-main', # Jan 15, 2022\n",
    "    '2022.emnlp-main', # Jun 24, 2022\n",
    "    '2023.acl', # Dec 15, 2022\n",
    "    '2023.emnlp-main', # Jun 23, 2023\n",
    "]\n",
    "\n",
    "for venue_index in venue_list:\n",
    "    print()\n",
    "    print(venue_index)\n",
    "    extract_full_text_and_save(venue_index)\n",
    "    print('Updating matched strings...')\n",
    "    update_matched_strings(f'data/{venue_index}_extracted_full.json', path='data/', option='model_names')\n",
    "    update_matched_strings(f'data/{venue_index}_extracted_full.json', path='data/', option='LM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cd5d4d-c56c-4bd3-9444-9a603f4a7ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
